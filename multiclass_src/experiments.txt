########## IRIS ##########
- Baseline: 
    Train - Epoch (59): | Acc: 0.9792 | W F1: 0.9792 | Micro F1: 1.0000 | Macro F1: 0.9788
    Test - Epoch (59): | Acc: 1.0000 | W F1: 1.0000 | Micro F1: 1.0000 | Macro F1: 1.0000
    Valid - Epoch (59): | Acc: 1.0000 | W F1: 1.0000 | Micro F1: 1.0000 | Macro F1: 1.0000
    Validation Loss: 0.5602492690086365
    EarlyStopping counter: 15 out of 15
    {'best-epoch': 59, 'loss': 0.675, 'f1_score': 1.0, 'accuracy': 1.0}

- Approx F1: 
    -- Mean Loss: 0.184
    Train - Epoch (59): | Acc: 0.9479 | W F1: 0.9479 | Micro F1: 0.9667 | Macro F1: 0.9470
    Test - Epoch (59): | Acc: 0.9667 | W F1: 0.9658 | Micro F1: 0.9667 | Macro F1: 0.9574
    Valid - Epoch (59): | Acc: 1.0000 | W F1: 1.0000 | Micro F1: 1.0000 | Macro F1: 1.0000
    Validation Loss: 0.004636585712432861
    EarlyStopping counter: 15 out of 15
    {'best-epoch': 59, 'loss': 0.1836, 'f1_score': 1.0, 'accuracy': 1.0}

- Approx Acc: 
    -- Mean Loss: 0.110
    Train - Epoch (81): | Acc: 0.9583 | W F1: 0.9584 | Micro F1: 0.9667 | Macro F1: 0.9577
    Test - Epoch (81): | Acc: 0.9667 | W F1: 0.9658 | Micro F1: 0.9667 | Macro F1: 0.9574
    Valid - Epoch (81): | Acc: 1.0000 | W F1: 1.0000 | Micro F1: 1.0000 | Macro F1: 1.0000
    Validation Loss: 0.0009115338325500488
    EarlyStopping counter: 15 out of 15
    {'best-epoch': 81, 'loss': 0.1103, 'f1_score': 1.0, 'accuracy': 1.0}





########## WINE ##########
- Baseline: 
    -- Mean Loss: 1.024
    Train - Epoch (123): | Acc: 0.775 | W F1: 0.763 | Micro F1: 0.6406 | Macro F1: 0.585
    Test - Epoch (123): | Acc: 0.641 | W F1: 0.630 | Micro F1: 0.6406 | Macro F1: 0.479
    Valid - Epoch (123): | Acc: 0.5927 | W F1: 0.5838 | Micro F1: 0.5838 | Macro F1: 0.5927
    Validation Loss: 1.1427412033081055
    EarlyStopping counter: 50 out of 50
    {'best-epoch': 123, 'loss': 1.0237, 'f1_score': 0.640, 'accuracy': 0.652}

- Approx F1 
    -- Mean Loss: 0.379
    Train - Epoch (183): | Acc: 0.700 | W F1: 0.699 | Macro F1: 0.678
    Test - Epoch (183): | Acc: 0.609 | W F1: 0.604 | Macro F1: 0.496
    Validation Loss: 0.4897085428237915
    EarlyStopping counter: 50 out of 50
    {'best-epoch': 183, 'loss': 0.3792, 'f1_score': 0.610, 'accuracy': 0.616}

- Approx AUROC 
    -- Mean Loss: 0.443
    Train - Epoch (43): | Acc: 0.757 | W F1: 0.756 | Micro F1: 0.6217 | Macro F1: 0.726
    Test - Epoch (43): | Acc: 0.622 | W F1: 0.618 | Micro F1: 0.6217 | Macro F1: 0.502
    Valid - Epoch (43): | Acc: 0.5927 | W F1: 0.5900 | Micro F1: 0.5900 | Macro F1: 0.5927
    Validation Loss: 0.47459903359413147
    EarlyStopping counter: 16 out of 50

Approx Accuracy 
    * Does not learn * 

    Train - Epoch (82): | Acc: 0.461 | W F1: 0.291 | Micro F1: 0.4576 | Macro F1: 0.158
    Test - Epoch (82): | Acc: 0.458 | W F1: 0.287 | Micro F1: 0.4576 | Macro F1: 0.157
    Valid - Epoch (82): | Acc: 0.4184 | W F1: 0.2468 | Micro F1: 0.2468 | Macro F1: 0.4184
    Validation Loss: 0.29092034697532654






########## CIFAR ##########
########## CIFAR BALANCED ##########
--- Baseline
    Train - Epoch (127): | Acc: 0.782 | W F1: 0.782 | Micro F1: 0.774| Macro F1: 0.782
    Test - Epoch (127): | Acc: 0.758 | W F1: 0.759 | Micro F1: 0.758 | Macro F1: 0.759
    Count of 9's in Preds: 4560 and Labels: 5000
                precision    recall  f1-score   support
            0       0.82      0.75      0.78      5000
            1       0.87      0.83      0.85      5000
            2       0.77      0.63      0.69      5000
            3       0.65      0.64      0.64      5000
            4       0.69      0.76      0.72      5000
            5       0.61      0.75      0.67      5000
            6       0.83      0.78      0.81      5000
            7       0.80      0.78      0.79      5000
            8       0.76      0.88      0.82      5000
            9       0.85      0.78      0.81      5000

     accuracy                           0.76     50000
    macro avg       0.76      0.76      0.76     50000
 weighted avg       0.76      0.76      0.76     50000
    EarlyStopping counter: 50 out of 50
    {'best-epoch': 127, 'loss': 1.7291, 'f1_score': 0.7717, 'accuracy': 0.77304}

--- Approx F1:
Train - Epoch (100): | Acc: 0.722 | W F1: 0.722 | Micro F1: 0.715| Macro F1: 0.722
Test - Epoch (100): | Acc: 0.712 | W F1: 0.714 | Micro F1: 0.712 | Macro F1: 0.714
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 5311 and Labels: 5000
              precision    recall  f1-score   support

           0       0.79      0.69      0.74      5000
           1       0.79      0.86      0.82      5000
           2       0.69      0.60      0.64      5000
           3       0.51      0.64      0.57      5000
           4       0.67      0.66      0.67      5000
           5       0.61      0.64      0.63      5000
           6       0.76      0.74      0.75      5000
           7       0.83      0.69      0.75      5000
           8       0.79      0.81      0.80      5000
           9       0.75      0.80      0.78      5000

    accuracy                           0.71     50000
   macro avg       0.72      0.71      0.71     50000
weighted avg       0.72      0.71      0.71     50000

EarlyStopping counter: 50 out of 50
{'best-epoch': 100, 'loss': 0.3349, 'f1_score': 0.7213847310754337, 'accuracy': 0.72156}

- Approx Accuracy
    * Doesn't learn * 

- Approx AUROC 
    * Doesn't learn * , need to fix to approximate trapezoidal better. 


########## CIFAR - Imbalanced (80-20) Oversampling ##########
--- Baseline 
Train - Epoch (121): | Acc: 0.670 | W F1: 0.670 | Micro F1: 0.660| Macro F1: 0.670
Test - Epoch (121): | Acc: 0.432 | W F1: 0.426 | Micro F1: 0.432 | Macro F1: 0.426
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 577 and Labels: 1000
              precision    recall  f1-score   support

           0       0.52      0.47      0.50      1000
           1       0.47      0.60      0.53      1000
           2       0.36      0.28      0.32      1000
           3       0.28      0.25      0.27      1000
           4       0.40      0.39      0.40      1000
           5       0.39      0.35      0.37      1000
           6       0.49      0.46      0.47      1000
           7       0.38      0.63      0.47      1000
           8       0.53      0.56      0.54      1000
           9       0.54      0.31      0.39      1000
    accuracy                           0.43     10000
   macro avg       0.44      0.43      0.43     10000
weighted avg       0.44      0.43      0.43     10000

    EarlyStopping counter: 50 out of 50
    {'best-epoch': 121, 'loss': 1.8728, 'f1_score': 0.445, 'accuracy': 0.4467}

Train - Epoch (88): | Acc: 0.662 | W F1: 0.662 | Micro F1: 0.652| Macro F1: 0.662
Test - Epoch (88): | Acc: 0.434 | W F1: 0.427 | Micro F1: 0.434 | Macro F1: 0.427
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 439 and Labels: 1000
              precision    recall  f1-score   support

           0       0.53      0.48      0.50      1000
           1       0.45      0.65      0.53      1000
           2       0.33      0.34      0.33      1000
           3       0.27      0.28      0.27      1000
           4       0.42      0.31      0.36      1000
           5       0.35      0.39      0.36      1000
           6       0.49      0.57      0.52      1000
           7       0.49      0.50      0.50      1000
           8       0.53      0.59      0.56      1000
           9       0.55      0.24      0.33      1000

    accuracy                           0.43     10000
   macro avg       0.44      0.43      0.43     10000
weighted avg       0.44      0.43      0.43     10000

EarlyStopping counter: 50 out of 50
Early Stopping
{'best-epoch': 88, 'loss': 1.8887, 'f1_score': 0.4436, 'accuracy': 0.4499}








--- Approx F1 
Train - Epoch (96): | Acc: 0.629 | W F1: 0.629 | Micro F1: 0.625| Macro F1: 0.629
Test - Epoch (96): | Acc: 0.421 | W F1: 0.415 | Micro F1: 0.421 | Macro F1: 0.415
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 227 and Labels: 1000
              precision    recall  f1-score   support

           0       0.51      0.54      0.52      1000
           1       0.50      0.56      0.53      1000
           2       0.32      0.32      0.32      1000
           3       0.27      0.34      0.30      1000
           4       0.32      0.40      0.35      1000
           5       0.36      0.35      0.35      1000
           6       0.47      0.51      0.49      1000
           7       0.49      0.49      0.49      1000
           8       0.52      0.56      0.54      1000
           9       0.67      0.15      0.25      1000

    accuracy                           0.42     10000
   macro avg       0.44      0.42      0.41     10000
weighted avg       0.44      0.42      0.41     10000

EarlyStopping counter: 50 out of 50
{'best-epoch': 96, 'loss': 0.4461, 'f1_score': 0.438, 'accuracy': 0.4375}

/* With a different seed */ 
Train - Epoch (78): | Acc: 0.633 | W F1: 0.633 | Micro F1: 0.628| Macro F1: 0.633
Test - Epoch (78): | Acc: 0.423 | W F1: 0.428 | Micro F1: 0.423 | Macro F1: 0.428
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 611 and Labels: 1000
              precision    recall  f1-score   support

           0       0.53      0.46      0.49      1000
           1       0.50      0.53      0.52      1000
           2       0.31      0.37      0.34      1000
           3       0.23      0.38      0.28      1000
           4       0.38      0.30      0.34      1000
           5       0.36      0.37      0.36      1000
           6       0.54      0.44      0.49      1000
           7       0.53      0.45      0.49      1000
           8       0.55      0.60      0.57      1000
           9       0.54      0.33      0.41      1000

    accuracy                           0.42     10000
   macro avg       0.45      0.42      0.43     10000
weighted avg       0.45      0.42      0.43     10000

EarlyStopping counter: 50 out of 50
{'best-epoch': 78, 'loss': 0.4566, 'f1_score': 0.43537846550551185, 'accuracy': 0.4361}

- Approx AUROC 

- Approx Acc 
    * Doesn't learn * 



########## CIFAR - Imbalanced (90-10) Oversampling ########## -> in 4th window 
--- Baseline 
Train - Epoch (174): | Acc: 0.633 | W F1: 0.633 | Micro F1: 0.623| Macro F1: 0.633
Test - Epoch (174): | Acc: 0.431 | W F1: 0.424 | Micro F1: 0.431 | Macro F1: 0.424
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 362 and Labels: 1000
              precision    recall  f1-score   support

           0       0.55      0.47      0.51      1000
           1       0.47      0.63      0.54      1000
           2       0.39      0.30      0.34      1000
           3       0.27      0.32      0.29      1000
           4       0.39      0.35      0.37      1000
           5       0.36      0.43      0.39      1000
           6       0.48      0.48      0.48      1000
           7       0.45      0.54      0.49      1000
           8       0.51      0.61      0.56      1000
           9       0.53      0.19      0.28      1000

    accuracy                           0.43     10000
   macro avg       0.44      0.43      0.42     10000
weighted avg       0.44      0.43      0.42     10000

EarlyStopping counter: 50 out of 50
{'best-epoch': 174, 'loss': 1.8658, 'f1_score': 0.4370, 'accuracy': 0.4431}

--- Approx F1 

Train - Epoch (82): | Acc: 0.623 | W F1: 0.623 | Micro F1: 0.622| Macro F1: 0.623
Test - Epoch (82): | Acc: 0.413 | W F1: 0.405 | Micro F1: 0.413 | Macro F1: 0.405
[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0]
Count of 9's in Preds: 197 and Labels: 1000
              precision    recall  f1-score   support

           0       0.58      0.45      0.51      1000
           1       0.44      0.64      0.52      1000
           2       0.32      0.36      0.34      1000
           3       0.25      0.38      0.30      1000
           4       0.38      0.35      0.36      1000
           5       0.35      0.31      0.33      1000
           6       0.44      0.48      0.46      1000
           7       0.49      0.45      0.47      1000
           8       0.55      0.59      0.57      1000
           9       0.58      0.12      0.19      1000

    accuracy                           0.41     10000
   macro avg       0.44      0.41      0.40     10000
weighted avg       0.44      0.41      0.40     10000

EarlyStopping counter: 50 out of 50
{'best-epoch': 82, 'loss': 0.4600, 'f1_score': 0.416, 'accuracy': 0.4198}

--- Approx AUROC 

--- Approx Acc 
    * Doesn't learn * 




########## MNIST ##########
MNIST Balanced: 
- Baseline
    Train - Epoch (189): | Acc: 0.9976 | W F1: 0.9976 | Micro F1: 0.9975| Macro F1: 0.9976
    Test - Epoch (189): | Acc: 0.9920 | W F1: 0.9920 | Micro F1: 0.9920 | Macro F1: 0.9920
    Count of 9's in Preds: 1000 and Labels: 1009
                precision    recall  f1-score   support

            0       0.99      0.99      0.99       980
            1       0.99      1.00      0.99      1135
            2       0.99      0.99      0.99      1032
            3       0.99      1.00      1.00      1010
            4       0.99      1.00      0.99       982
            5       0.99      0.99      0.99       892
            6       1.00      0.99      0.99       958
            7       0.99      0.99      0.99      1028
            8       0.99      0.99      0.99       974
            9       0.99      0.98      0.99      1009

        accuracy                           0.99     10000
    macro avg       0.99      0.99      0.99     10000
    weighted avg       0.99      0.99      0.99     10000
    EarlyStopping counter: 50 out of 50
    {'best-epoch': 189, 'loss': 1.4676, 'f1_score': 0.9927963456641932, 'accuracy': 0.9928}

- Approx Acc 

- Approx F1 

- Approx AUROC 



MNIST Imbalanced 80-20 
- Baseline 

- Approx F1 

- Approx Acc 

- Approx AUROC 


