TODO's 
- Analyze results that used a larger batch size, and see if there was a change in results. 
- Analyze results from the baseline, because used test vs. validation. 
- Analyze results from evaluating on different thresholds, when trained on 0.5. 







Results so far: 
- 128 batch size - trained on the range of thresholds. 
- 512 batch size - trained on the range of thresholds. 
- 1024 batch size - trained on the range of thresholds. 
- 2048 batch size - trained on the range of thresholds. 
- Runs with the wt-approx (dividing by negative classes) 
- 1024 runs, trained on 0.5, evaluated on other thresholds. 

__________________________

LOG 

11/24 
- Running 2048 balanced experiments for CIFAR.
- Running 2048 adjusted approx for CIFAR.  

11/23 
- Running 512 balanced, imbalanced for CIFAR. 


11/19 
- Because we're training at 0.5; the sigmoid for high k works well. As we change the evaluation threshold, 
the precision and recall values pretty much flip; best precision happens at the highest threshold. Recall is better 
when we have more output. 


The goal is to train at all thresholds, and compare them in this confusion matrix style. 
Rows are the training thresholds, and columns are the evaluation thresholds. 
    Every row is a network, trained from 0.1 to 0.9     
    Each column corresponds to a threshold that you're predicting at. 


We can incorporate, in the validation step, moving the threshold in the right direction. 
If that works, it'd be really interesting. 