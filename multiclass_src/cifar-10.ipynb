{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, ConcatDataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(train=True, download=True,\n",
    "                  root=\"../data\", transform=transform)\n",
    "test_data = CIFAR10(train=False, download=True,\n",
    "                    root=\"../data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_balance(dataset): \n",
    "    targets = np.array(dataset.targets)\n",
    "    classes, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(classes)\n",
    "    print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalance(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # transform - transforms data during creation, downloads it locally, stores it in root, is train \n",
    "dataset = CIFAR10(train=True, download=True, root=\"../data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 46000\n",
       "    Root location: ../data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sizes(dataset): \n",
    "    sizes = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "    for d in dataset.targets: \n",
    "        sizes[d] += 1\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5000,\n",
       " 1: 5000,\n",
       " 2: 5000,\n",
       " 3: 5000,\n",
       " 4: 5000,\n",
       " 5: 5000,\n",
       " 6: 5000,\n",
       " 7: 5000,\n",
       " 8: 5000,\n",
       " 9: 1000}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sizes(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalance(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    print(\"imb class idx: {}\".format(imbal_class_indices))\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "    \n",
    "    print(len(dataset.targets))\n",
    "    print(dataset.targets[0:10])\n",
    "    print(len(dataset.data))\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset \n",
    "\n",
    "def create_oversample(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    print(\"imb class idx: {}\".format(imbal_class_indices))\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "    \n",
    "    print(len(dataset.targets))\n",
    "    print(dataset.targets[0:10])\n",
    "    print(len(dataset.data))\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "imb class idx: [    0     1     2 ... 45997 45998 45999]\n",
      "46000\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "46000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 5000,\n",
       " 1: 5000,\n",
       " 2: 5000,\n",
       " 3: 5000,\n",
       " 4: 5000,\n",
       " 5: 5000,\n",
       " 6: 5000,\n",
       " 7: 5000,\n",
       " 8: 5000,\n",
       " 9: 1000}"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imb_dataset = create_imbalance(dataset)\n",
    "check_sizes(imb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n"
     ]
    }
   ],
   "source": [
    "target = imb_dataset.targets\n",
    "class_sample_count = np.unique(target, return_counts=True)[1]\n",
    "print(class_sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "       0.0002, 0.001 ])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 1. / class_sample_count\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_weight = weight[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46000"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1. / class_sample_count\n",
    "samples_weight = weight[target]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    imb_dataset, batch_size=10, num_workers=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 1000])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00025, 0.0002 , 0.0002 , 0.0002 , 0.0002 , 0.0002 , 0.0002 ,\n",
       "       0.0002 , 0.0002 , 0.001  ])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002, 0.0010, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "        0.0010], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, classes [0], count [10]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print('Batch {}, classes {}, count {}'.format(\n",
    "        batch_idx, *np.unique(target.numpy(), return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random(n):\n",
    "    random_idx = [] \n",
    "    for i in range(n):\n",
    "        random_idx.append(random.randint(0,999))\n",
    "    print(\"{}\".format(len(random_idx)))\n",
    "    return random_idx \n",
    "\n",
    "indices = generate_random(4000)\n",
    "class_nine = [] \n",
    "for i in range(len(imb_dataset)): \n",
    "    if imb_dataset.__getitem__(i)[1] == 9:\n",
    "        class_nine.append(imb_dataset.__getitem__(i))\n",
    "\n",
    "samples = [] \n",
    "for idx in indices: \n",
    "    samples.append(class_nine[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_x = np.array([samples[i][0].numpy() for i in range(len(samples))])\n",
    "samples_y = [samples[i][1] for i in range(len(samples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting OG dataset to arrays \n",
    "og_dataset = [] \n",
    "for i in range(len(imb_dataset)): \n",
    "    new_tuple = (imb_dataset[i][0].numpy(), imb_dataset[i][1])\n",
    "    og_dataset.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = [] \n",
    "for i in range(len(samples_x)):\n",
    "    new_tuple = (samples_x[i], samples_y[i])\n",
    "    sampled_dataset.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.append(og_dataset, sampled_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(show=False, imbalanced=None):\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # transform - transforms data during creation, downloads it locally, stores it in root, is train \n",
    "    dataset = CIFAR10(train=True, download=True, root=\"../data\", transform=transform)\n",
    "    test_data = CIFAR10(train=False, download=True, root=\"../data\", transform=transform)\n",
    "    print(\"Train Size: {}, Test Size: {}\".format(len(dataset), len(test_data)))\n",
    "\n",
    "    val_size = 5000\n",
    "    if imbalanced:\n",
    "        dataset = create_imbalance(dataset)\n",
    "        val_size = 4600  # dataset is now 46000\n",
    "    \n",
    "    print(\"Train Size: {}, Test Size: {}\".format(len(dataset), len(test_data)))\n",
    "\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    # Splitting into train/test/validation\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # forming batches, putting into loader:\n",
    "    batch_size = 128 \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    # loading the dataset --> DataLoader class (torch.utils.data.DataLoader)\n",
    "    classes = dataset.classes \n",
    "    print(\"Classes: {}\".format(classes))\n",
    "\n",
    "    # showing image\n",
    "    if show: \n",
    "        # getting random training images \n",
    "        dataiter = iter(train_loader)\n",
    "        images, labels = dataiter.next() \n",
    "\n",
    "        # showing images \n",
    "        show_image(torchvision.utils.make_grid(images))\n",
    "        print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Size: 50000, Test Size: 10000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n",
      "Train Size: 46000, Test Size: 10000\n",
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_data(show=False, imbalanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1418068b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CHECKING TRAIN ------\n",
      "tensor([3, 6, 7, 4, 7, 1, 6, 7, 7, 8, 3, 5, 5, 5, 5, 0, 2, 2, 0, 6, 7, 5, 4, 0,\n",
      "        5, 5, 0, 4, 6, 7, 0, 3, 6, 6, 7, 1, 2, 2, 8, 0, 4, 7, 4, 3, 1, 2, 8, 7,\n",
      "        2, 1, 7, 4, 5, 0, 6, 6, 8, 7, 3, 6, 8, 6, 0, 4, 7, 7, 1, 5, 1, 3, 7, 7,\n",
      "        1, 5, 6, 7, 1, 6, 1, 0, 7, 7, 1, 3, 2, 5, 0, 7, 8, 9, 6, 2, 7, 5, 1, 8,\n",
      "        7, 7, 1, 3, 2, 3, 4, 1, 1, 0, 6, 1, 0, 5, 0, 6, 4, 2, 0, 8, 1, 8, 3, 5,\n",
      "        6, 2, 1, 3, 2, 7, 7, 5])\n",
      "tensor([1, 5, 7, 6, 0, 3, 5, 3, 5, 2, 5, 7, 8, 5, 7, 1, 2, 4, 9, 5, 0, 5, 3, 3,\n",
      "        6, 7, 5, 6, 8, 3, 7, 6, 7, 6, 4, 5, 3, 6, 7, 3, 4, 4, 0, 5, 0, 6, 6, 9,\n",
      "        6, 8, 8, 4, 2, 5, 1, 3, 6, 2, 3, 2, 4, 5, 0, 1, 2, 5, 3, 8, 5, 2, 1, 4,\n",
      "        8, 7, 7, 3, 6, 3, 5, 1, 0, 4, 6, 5, 8, 1, 2, 9, 0, 2, 0, 3, 0, 8, 1, 7,\n",
      "        4, 7, 9, 7, 8, 7, 5, 6, 2, 8, 5, 0, 4, 1, 4, 1, 6, 3, 4, 6, 6, 8, 3, 1,\n",
      "        6, 8, 0, 2, 7, 5, 6, 2])\n",
      "tensor([7, 2, 3, 3, 8, 2, 8, 2, 2, 2, 3, 8, 5, 7, 0, 7, 0, 6, 5, 6, 0, 5, 0, 0,\n",
      "        2, 3, 3, 1, 9, 6, 6, 2, 2, 2, 1, 8, 9, 0, 4, 1, 7, 1, 0, 5, 1, 0, 4, 6,\n",
      "        4, 8, 3, 6, 5, 0, 5, 9, 6, 1, 5, 5, 7, 6, 3, 7, 3, 9, 7, 2, 5, 5, 1, 7,\n",
      "        1, 5, 1, 4, 6, 1, 0, 3, 4, 3, 7, 6, 6, 7, 7, 0, 5, 1, 0, 7, 8, 7, 5, 2,\n",
      "        8, 6, 3, 3, 0, 6, 7, 5, 9, 7, 5, 6, 2, 4, 2, 7, 8, 4, 8, 4, 8, 6, 2, 5,\n",
      "        8, 3, 4, 1, 3, 9, 9, 3])\n",
      "tensor([4, 2, 1, 1, 4, 6, 1, 1, 5, 8, 2, 1, 0, 7, 8, 3, 3, 1, 0, 3, 5, 0, 5, 7,\n",
      "        5, 0, 0, 5, 1, 3, 8, 4, 5, 8, 8, 0, 3, 1, 2, 8, 4, 4, 1, 7, 9, 2, 8, 3,\n",
      "        0, 8, 8, 7, 0, 8, 8, 1, 1, 5, 3, 2, 5, 7, 4, 0, 7, 2, 8, 3, 7, 4, 2, 4,\n",
      "        0, 6, 8, 5, 6, 4, 3, 0, 0, 7, 2, 1, 8, 3, 7, 6, 7, 2, 3, 4, 0, 3, 2, 4,\n",
      "        6, 5, 6, 4, 2, 7, 4, 7, 7, 6, 2, 8, 2, 3, 6, 8, 2, 0, 2, 7, 5, 7, 8, 6,\n",
      "        5, 6, 3, 0, 7, 1, 6, 0])\n",
      "------ CHECKING TEST ------\n",
      "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
      "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
      "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
      "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
      "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
      "        8, 3, 1, 2, 8, 0, 8, 3])\n",
      "tensor([5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9,\n",
      "        6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9,\n",
      "        3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8,\n",
      "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4,\n",
      "        3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9,\n",
      "        2, 2, 5, 3, 9, 9, 4, 0])\n",
      "tensor([3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3,\n",
      "        9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2,\n",
      "        6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5,\n",
      "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1,\n",
      "        0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7,\n",
      "        0, 7, 0, 6, 6, 9, 0, 9])\n",
      "tensor([2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
      "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
      "        3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
      "        3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4,\n",
      "        8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
      "        9, 4, 7, 9, 9, 4, 5, 6])\n",
      "------ CHECKING VALIDATION ------\n",
      "tensor([9, 8, 3, 3, 3, 5, 2, 1, 7, 5, 5, 2, 2, 3, 4, 4, 5, 3, 5, 1, 0, 4, 3, 1,\n",
      "        6, 0, 4, 3, 6, 1, 4, 6, 8, 3, 5, 6, 8, 8, 0, 6, 1, 2, 2, 7, 0, 1, 6, 6,\n",
      "        2, 8, 0, 7, 2, 3, 7, 8, 4, 9, 8, 8, 5, 1, 9, 5, 2, 0, 6, 3, 2, 5, 1, 3,\n",
      "        2, 7, 7, 5, 8, 8, 4, 6, 0, 4, 8, 1, 1, 7, 0, 1, 7, 5, 4, 0, 5, 5, 6, 4,\n",
      "        8, 3, 6, 5, 9, 2, 3, 0, 6, 9, 0, 0, 1, 5, 0, 0, 9, 7, 1, 3, 8, 1, 5, 6,\n",
      "        1, 8, 6, 1, 2, 3, 4, 3])\n",
      "tensor([2, 8, 8, 4, 1, 5, 0, 2, 5, 1, 1, 3, 6, 6, 0, 4, 8, 4, 1, 3, 0, 3, 6, 4,\n",
      "        0, 1, 1, 5, 0, 2, 6, 9, 0, 7, 6, 7, 6, 2, 2, 2, 1, 5, 0, 0, 3, 3, 6, 3,\n",
      "        6, 2, 2, 1, 3, 7, 3, 1, 0, 7, 7, 0, 3, 0, 8, 0, 1, 7, 4, 0, 7, 8, 1, 1,\n",
      "        9, 2, 8, 7, 1, 8, 5, 4, 4, 5, 1, 0, 0, 8, 1, 2, 2, 8, 1, 7, 0, 3, 2, 7,\n",
      "        2, 7, 6, 3, 4, 0, 4, 2, 4, 3, 5, 8, 3, 3, 2, 2, 4, 0, 4, 2, 7, 4, 5, 3,\n",
      "        3, 3, 0, 2, 3, 6, 1, 7])\n",
      "tensor([6, 1, 8, 8, 3, 5, 7, 9, 8, 5, 3, 8, 2, 5, 5, 4, 0, 2, 1, 6, 6, 6, 0, 5,\n",
      "        5, 8, 0, 3, 0, 1, 8, 4, 7, 4, 7, 0, 3, 6, 3, 6, 3, 1, 8, 5, 8, 0, 1, 6,\n",
      "        1, 7, 6, 3, 6, 7, 2, 4, 8, 1, 5, 6, 7, 3, 3, 8, 0, 8, 0, 2, 4, 7, 8, 6,\n",
      "        2, 1, 2, 3, 4, 4, 3, 0, 7, 6, 8, 7, 8, 1, 8, 8, 0, 5, 5, 5, 2, 0, 8, 8,\n",
      "        8, 8, 7, 1, 4, 5, 1, 7, 0, 7, 1, 0, 0, 0, 3, 1, 5, 0, 3, 8, 1, 8, 6, 4,\n",
      "        0, 7, 8, 0, 1, 1, 7, 2])\n",
      "tensor([8, 0, 6, 3, 8, 0, 6, 2, 8, 5, 2, 7, 9, 7, 3, 0, 6, 5, 6, 5, 5, 5, 8, 6,\n",
      "        2, 2, 7, 4, 7, 6, 3, 3, 4, 6, 6, 4, 4, 8, 6, 5, 1, 8, 3, 3, 0, 4, 8, 2,\n",
      "        6, 2, 4, 5, 1, 3, 0, 2, 3, 5, 4, 5, 9, 4, 0, 4, 3, 8, 2, 6, 9, 6, 6, 6,\n",
      "        5, 3, 2, 8, 2, 5, 3, 1, 4, 1, 4, 8, 6, 3, 5, 2, 3, 4, 1, 4, 2, 5, 0, 1,\n",
      "        3, 5, 4, 8, 3, 6, 8, 3, 9, 7, 1, 6, 1, 4, 6, 0, 3, 0, 9, 9, 3, 2, 2, 6,\n",
      "        1, 7, 2, 2, 1, 8, 9, 0])\n"
     ]
    }
   ],
   "source": [
    "# print(\"------ CHECKING TRAIN ------\")\n",
    "# for i, (inputs, labels) in enumerate(train_loader):\n",
    "#     if i > 3: \n",
    "#         break\n",
    "#     print(labels)\n",
    "    \n",
    "# print(\"------ CHECKING TEST ------\")\n",
    "# for i, (inputs, labels) in enumerate(test_loader):\n",
    "#     if i > 3: \n",
    "#         break\n",
    "#     print(labels)\n",
    "\n",
    "# print(\"------ CHECKING VALIDATION ------\")\n",
    "# for i, (inputs, labels) in enumerate(val_loader):\n",
    "#     if i > 3: \n",
    "#         break\n",
    "#     print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    torch.manual_seed(1)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    train_data = CIFAR10(train=True, download=True,\n",
    "                      root=\"../data\", transform=transform)\n",
    "\n",
    "    # creating imbalance for class #10 for the train data. \n",
    "    imbalanced_train = create_imbalance(train_data)\n",
    "\n",
    "    # splitting into a train and a validation \n",
    "    # creating the weighted sampler for both of these \n",
    "    # validation set is 4600 (10% of train), Train is 90%. \n",
    "    train_ds, val_ds = random_split(imbalanced_train, [41400, 4600])\n",
    "\n",
    "    # create sampler\n",
    "    sampler = adjust_imbalance_sampler(imbalanced_train)\n",
    "    train_loader = DataLoader(train_ds, batch_size=200, num_workers=4, sampler=sampler)\n",
    "    val_loader = DataLoader(val_ds, batch_size=200, sampler=sampler, num_workers=4)\n",
    "\n",
    "    test_data = CIFAR10(train=False, download=True,\n",
    "                        root=\"../data\", transform=transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_data, batch_size=200, num_workers=4, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_imbalance_sampler(dataset):\n",
    "    print(dataset)\n",
    "    targets = dataset.targets\n",
    "    class_count = np.unique(targets, return_counts=True)[1]\n",
    "    print(class_count)\n",
    "\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = weight[targets]\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n",
      "imb class idx: [   29    30    35 ... 10178 10187 10198]\n",
      "46000\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "46000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 46000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x144a90880>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# manually normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from download_cifar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "Images shape: (50000, 32, 32, 3)\n",
      "Sample Image: [[[ 59.  62.  63.]\n",
      "  [ 43.  46.  45.]\n",
      "  [ 50.  48.  43.]\n",
      "  ...\n",
      "  [158. 132. 108.]\n",
      "  [152. 125. 102.]\n",
      "  [148. 124. 103.]]\n",
      "\n",
      " [[ 16.  20.  20.]\n",
      "  [  0.   0.   0.]\n",
      "  [ 18.   8.   0.]\n",
      "  ...\n",
      "  [123.  88.  55.]\n",
      "  [119.  83.  50.]\n",
      "  [122.  87.  57.]]\n",
      "\n",
      " [[ 25.  24.  21.]\n",
      "  [ 16.   7.   0.]\n",
      "  [ 49.  27.   8.]\n",
      "  ...\n",
      "  [118.  84.  50.]\n",
      "  [120.  84.  50.]\n",
      "  [109.  73.  42.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208. 170.  96.]\n",
      "  [201. 153.  34.]\n",
      "  [198. 161.  26.]\n",
      "  ...\n",
      "  [160. 133.  70.]\n",
      "  [ 56.  31.   7.]\n",
      "  [ 53.  34.  20.]]\n",
      "\n",
      " [[180. 139.  96.]\n",
      "  [173. 123.  42.]\n",
      "  [186. 144.  30.]\n",
      "  ...\n",
      "  [184. 148.  94.]\n",
      "  [ 97.  62.  34.]\n",
      "  [ 83.  53.  34.]]\n",
      "\n",
      " [[177. 144. 116.]\n",
      "  [168. 129.  94.]\n",
      "  [179. 142.  87.]\n",
      "  ...\n",
      "  [216. 184. 140.]\n",
      "  [151. 118.  84.]\n",
      "  [123.  92.  72.]]]\n",
      "[0.49139968 0.48215841 0.44653091]\n",
      "[0.24703223 0.24348513 0.26158784]\n",
      "Image shape: (3, 32, 32)\n",
      "Length of Train: 50000\n",
      "Length of Labels: 50000\n",
      "Class 9: 5000\n"
     ]
    }
   ],
   "source": [
    "datasplits = load_imb_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = datasplits['train']['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transposing \n",
    "reshape_x_train = x_train[0].reshape([32, 32, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12b6cdf10>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL0klEQVR4nO3dX4ilhXnH8e+v/mlLFKLd6bKs2k2stHjRrDIslkhIkxqsNyqUohfBC2FDiaCQXkgKrYVemFKVXhTLWiVLsVpbFZcibawIEgjG0a7r6rbVyIa4rLsjVrQ3TdWnF+ddmJWZndk5/zZ5vh8Y5pz3vGffh5f9zpzzzuF9U1VI+vn3C/MeQNJsGLvUhLFLTRi71ISxS00Yu9TE2eM8Ocm1wF8BZwF/W1V3n2r9LVu21I4dO8bZpKRTOHz4MO+++25We2zTsSc5C/hr4BrgbeDFJPuq6vW1nrNjxw6WlpY2u0lJ61hcXFzzsXFexu8C3qyqt6rqp8CjwPVj/HuSpmic2LcDP1lx/+1hmaQz0NQP0CXZnWQpydLy8vK0NydpDePEfgS4eMX9i4ZlJ6mqPVW1WFWLCwsLY2xO0jjGif1F4LIkn0tyLnATsG8yY0matE0fja+qj5LcBvwroz+9PVRVr01sMkkTNdbf2avqaeDpCc0iaYr8BJ3UhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUxFhXhElyGPgQ+Bj4qKrWvhK8pLkaK/bB71TVuxP4dyRNkS/jpSbGjb2A7yV5KcnuSQwkaTrGfRl/dVUdSfKrwDNJ/qOqnl+5wvBDYDfAJZdcMubmJG3WWL/Zq+rI8P048CSwa5V19lTVYlUtLiwsjLM5SWPYdOxJPpPk/BO3ga8BByc1mKTJGudl/FbgySQn/p2/r6p/mchUkiZu07FX1VvAFyY4i6Qp8k9vUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPrxp7koSTHkxxcsezCJM8keWP4fsF0x5Q0ro38Zv8ucO2nlt0JPFtVlwHPDvclncHWjX243vp7n1p8PbB3uL0XuGGyY0matM2+Z99aVUeH2+8wuqKrpDPY2AfoqqqAWuvxJLuTLCVZWl5eHndzkjZps7EfS7INYPh+fK0Vq2pPVS1W1eLCwsImNydpXJuNfR9wy3D7FuCpyYwjaVo28qe3R4AfAL+R5O0ktwJ3A9ckeQP43eG+pDPY2eutUFU3r/HQVyc8i6Qp8hN0UhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhMbufzTQ0mOJzm4YtldSY4k2T98XTfdMSWNayO/2b8LXLvK8vuqaufw9fRkx5I0aevGXlXPA+/NYBZJUzTOe/bbkhwYXuZfMLGJJE3FZmO/H7gU2AkcBe5Za8Uku5MsJVlaXl7e5OYkjWtTsVfVsar6uKo+AR4Adp1i3T1VtVhViwsLC5udU9KYNhV7km0r7t4IHFxrXUlnhrPXWyHJI8CXgS1J3gb+FPhykp1AAYeBb0xvREmTsG7sVXXzKosfnMIskqbIT9BJTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTawbe5KLkzyX5PUkryW5fVh+YZJnkrwxfPeyzdIZbCO/2T8CvlVVlwNXAd9McjlwJ/BsVV0GPDvcl3SGWjf2qjpaVS8Ptz8EDgHbgeuBvcNqe4EbpjSjpAk4rffsSXYAVwAvAFur6ujw0DvA1smOJmmSNhx7kvOAx4E7quqDlY9VVTG6fPNqz9udZCnJ0vLy8ljDStq8DcWe5BxGoT9cVU8Mi48l2TY8vg04vtpzq2pPVS1W1eLCwsIkZpa0CRs5Gh9G12M/VFX3rnhoH3DLcPsW4KnJjydpUs7ewDpfBL4OvJpk/7Ds28DdwGNJbgV+DPzBVCaUNBHrxl5V3weyxsNfnew4kqbFT9BJTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTWzkWm8XJ3kuyetJXkty+7D8riRHkuwfvq6b/riSNmsj13r7CPhWVb2c5HzgpSTPDI/dV1V/Ob3xJE3KRq71dhQ4Otz+MMkhYPu0B5M0Waf1nj3JDuAK4IVh0W1JDiR5KMkFkx5O0uRsOPYk5wGPA3dU1QfA/cClwE5Gv/nvWeN5u5MsJVlaXl4ef2JJm7Kh2JOcwyj0h6vqCYCqOlZVH1fVJ8ADwK7VnltVe6pqsaoWFxYWJjW3pNO0kaPxAR4EDlXVvSuWb1ux2o3AwcmPJ2lSNnI0/ovA14FXk+wfln0buDnJTqCAw8A3pjCfpAnZyNH47wNZ5aGnJz+OpGnxE3RSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSExu51tsvJflhkleSvJbkz4bln0vyQpI3k/xDknOnP66kzdrIb/b/Bb5SVV9gdHnma5NcBXwHuK+qfh34b+DWqU0paWzrxl4j/zPcPWf4KuArwD8Ny/cCN0xjQEmTsdHrs581XMH1OPAM8CPg/ar6aFjlbWD7VCaUNBEbir2qPq6qncBFwC7gNze6gSS7kywlWVpeXt7clJLGdlpH46vqfeA54LeBzyY5ccnni4AjazxnT1UtVtXiwsLCOLNKGsNGjsYvJPnscPuXgWuAQ4yi//1htVuAp6Y0o6QJOHv9VdgG7E1yFqMfDo9V1T8neR14NMmfA/8OPDjFOSWNad3Yq+oAcMUqy99i9P5d0s8AP0EnNWHsUhPGLjVh7FITxi41kaqa3caSZeDHw90twLsz2/janONkznGyn7U5fq2qVv302kxjP2nDyVJVLc5l487hHA3n8GW81ISxS03MM/Y9c9z2Ss5xMuc42c/NHHN7zy5ptnwZLzUxl9iTXJvkP4eTVd45jxmGOQ4neTXJ/iRLM9zuQ0mOJzm4YtmFSZ5J8sbw/YI5zXFXkiPDPtmf5LoZzHFxkueSvD6c1PT2YflM98kp5pjpPpnaSV6raqZfwFmMTmv1eeBc4BXg8lnPMcxyGNgyh+1+CbgSOLhi2V8Adw637wS+M6c57gL+aMb7Yxtw5XD7fOC/gMtnvU9OMcdM9wkQ4Lzh9jnAC8BVwGPATcPyvwH+8HT+3Xn8Zt8FvFlVb1XVT4FHgevnMMfcVNXzwHufWnw9oxN3woxO4LnGHDNXVUer6uXh9oeMTo6ynRnvk1PMMVM1MvGTvM4j9u3AT1bcn+fJKgv4XpKXkuye0wwnbK2qo8Ptd4Ctc5zltiQHhpf5U387sVKSHYzOn/ACc9wnn5oDZrxPpnGS1+4H6K6uqiuB3wO+meRL8x4IRj/ZGf0gmof7gUsZXSPgKHDPrDac5DzgceCOqvpg5WOz3CerzDHzfVJjnOR1LfOI/Qhw8Yr7a56sctqq6sjw/TjwJPM9886xJNsAhu/H5zFEVR0b/qN9AjzAjPZJknMYBfZwVT0xLJ75Plltjnntk2Hb73OaJ3ldyzxifxG4bDiyeC5wE7Bv1kMk+UyS80/cBr4GHDz1s6ZqH6MTd8IcT+B5Iq7BjcxgnyQJo3MYHqqqe1c8NNN9stYcs94nUzvJ66yOMH7qaON1jI50/gj44znN8HlGfwl4BXhtlnMAjzB6Ofh/jN573Qr8CvAs8Abwb8CFc5rj74BXgQOMYts2gzmuZvQS/QCwf/i6btb75BRzzHSfAL/F6CSuBxj9YPmTFf9nfwi8Cfwj8Iun8+/6CTqpie4H6KQ2jF1qwtilJoxdasLYpSaMXWrC2KUmjF1q4v8BqWX3Q3KPAa0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(reshape_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
