{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import click\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split, ConcatDataset, WeightedRandomSampler\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = CIFAR10(train=True, download=True,\n",
    "                  root=\"../data\", transform=transform)\n",
    "test_data = CIFAR10(train=False, download=True,\n",
    "                    root=\"../data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['airplane',\n",
       " 'automobile',\n",
       " 'bird',\n",
       " 'cat',\n",
       " 'deer',\n",
       " 'dog',\n",
       " 'frog',\n",
       " 'horse',\n",
       " 'ship',\n",
       " 'truck']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_balance(dataset): \n",
    "    targets = np.array(dataset.targets)\n",
    "    classes, class_counts = np.unique(targets, return_counts=True)\n",
    "    nb_classes = len(classes)\n",
    "    print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalance(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # transform - transforms data during creation, downloads it locally, stores it in root, is train \n",
    "dataset = CIFAR10(train=True, download=True, root=\"../data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 46000\n",
       "    Root location: ../data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
       "           )"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.targets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sizes(dataset): \n",
    "    sizes = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "    for d in dataset.targets: \n",
    "        sizes[d] += 1\n",
    "    return sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 5000,\n",
       " 1: 5000,\n",
       " 2: 5000,\n",
       " 3: 5000,\n",
       " 4: 5000,\n",
       " 5: 5000,\n",
       " 6: 5000,\n",
       " 7: 5000,\n",
       " 8: 5000,\n",
       " 9: 1000}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_sizes(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_imbalance(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    print(\"imb class idx: {}\".format(imbal_class_indices))\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "    \n",
    "    print(len(dataset.targets))\n",
    "    print(dataset.targets[0:10])\n",
    "    print(len(dataset.data))\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset \n",
    "\n",
    "def create_oversample(dataset): \n",
    "    check_class_balance(dataset)\n",
    "    targets = np.array(dataset.targets)\n",
    "    # Create artificial imbalanced class counts, one of the classes has 805 of observations removed\n",
    "    # We sample from the class that doesn't have many classes \n",
    "    imbal_class_counts = [5000,5000,5000,5000,5000,5000,5000,5000,5000,1000]\n",
    "\n",
    "    # Get class indices\n",
    "    class_indices = [np.where(targets == i)[0] for i in range(10)]\n",
    "\n",
    "    # Get imbalanced number of instances\n",
    "    imbal_class_indices = [class_idx[:class_count] for class_idx, class_count in zip(class_indices, imbal_class_counts)]\n",
    "    imbal_class_indices = np.hstack(imbal_class_indices)\n",
    "\n",
    "    print(\"imb class idx: {}\".format(imbal_class_indices))\n",
    "    # Set target and data to dataset\n",
    "    dataset.targets = targets[imbal_class_indices]\n",
    "    dataset.data = dataset.data[imbal_class_indices]\n",
    "    \n",
    "    print(len(dataset.targets))\n",
    "    print(dataset.targets[0:10])\n",
    "    print(len(dataset.data))\n",
    "\n",
    "    assert len(dataset.targets) == len(dataset.data)\n",
    "    print(\"After imbalance: {}\".format(check_class_balance(dataset)))\n",
    "\n",
    "    return dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "imb class idx: [    0     1     2 ... 45997 45998 45999]\n",
      "46000\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "46000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 5000,\n",
       " 1: 5000,\n",
       " 2: 5000,\n",
       " 3: 5000,\n",
       " 4: 5000,\n",
       " 5: 5000,\n",
       " 6: 5000,\n",
       " 7: 5000,\n",
       " 8: 5000,\n",
       " 9: 1000}"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imb_dataset = create_imbalance(dataset)\n",
    "check_sizes(imb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchvision.datasets.cifar.CIFAR10"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(imb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n"
     ]
    }
   ],
   "source": [
    "target = imb_dataset.targets\n",
    "class_sample_count = np.unique(target, return_counts=True)[1]\n",
    "print(class_sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "       0.0002, 0.001 ])"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 1. / class_sample_count\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_weight = weight[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46000"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = 1. / class_sample_count\n",
    "samples_weight = weight[target]\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    imb_dataset, batch_size=10, num_workers=1, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 5000, 1000])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00025, 0.0002 , 0.0002 , 0.0002 , 0.0002 , 0.0002 , 0.0002 ,\n",
       "       0.0002 , 0.0002 , 0.001  ])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0002, 0.0010, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0003, 0.0002,\n",
      "        0.0010], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(samples_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, classes [0], count [10]\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    print('Batch {}, classes {}, count {}'.format(\n",
    "        batch_idx, *np.unique(target.numpy(), return_counts=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random(n):\n",
    "    random_idx = [] \n",
    "    for i in range(n):\n",
    "        random_idx.append(random.randint(0,999))\n",
    "    print(\"{}\".format(len(random_idx)))\n",
    "    return random_idx \n",
    "\n",
    "indices = generate_random(4000)\n",
    "class_nine = [] \n",
    "for i in range(len(imb_dataset)): \n",
    "    if imb_dataset.__getitem__(i)[1] == 9:\n",
    "        class_nine.append(imb_dataset.__getitem__(i))\n",
    "\n",
    "samples = [] \n",
    "for idx in indices: \n",
    "    samples.append(class_nine[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_x = np.array([samples[i][0].numpy() for i in range(len(samples))])\n",
    "samples_y = [samples[i][1] for i in range(len(samples))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting OG dataset to arrays \n",
    "og_dataset = [] \n",
    "for i in range(len(imb_dataset)): \n",
    "    new_tuple = (imb_dataset[i][0].numpy(), imb_dataset[i][1])\n",
    "    og_dataset.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = [] \n",
    "for i in range(len(samples_x)):\n",
    "    new_tuple = (samples_x[i], samples_y[i])\n",
    "    sampled_dataset.append(new_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.append(og_dataset, sampled_dataset, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(show=False, imbalanced=None):\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # transform - transforms data during creation, downloads it locally, stores it in root, is train \n",
    "    dataset = CIFAR10(train=True, download=True, root=\"../data\", transform=transform)\n",
    "    test_data = CIFAR10(train=False, download=True, root=\"../data\", transform=transform)\n",
    "    print(\"Train Size: {}, Test Size: {}\".format(len(dataset), len(test_data)))\n",
    "\n",
    "    val_size = 5000\n",
    "    if imbalanced:\n",
    "        dataset = create_imbalance(dataset)\n",
    "        val_size = 4600  # dataset is now 46000\n",
    "    \n",
    "    print(\"Train Size: {}, Test Size: {}\".format(len(dataset), len(test_data)))\n",
    "\n",
    "    train_size = len(dataset) - val_size\n",
    "\n",
    "    # Splitting into train/test/validation\n",
    "    train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # forming batches, putting into loader:\n",
    "    batch_size = 128 \n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, num_workers=4)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "    # loading the dataset --> DataLoader class (torch.utils.data.DataLoader)\n",
    "    classes = dataset.classes \n",
    "    print(\"Classes: {}\".format(classes))\n",
    "\n",
    "    # showing image\n",
    "    if show: \n",
    "        # getting random training images \n",
    "        dataiter = iter(train_loader)\n",
    "        images, labels = dataiter.next() \n",
    "\n",
    "        # showing images \n",
    "        show_image(torchvision.utils.make_grid(images))\n",
    "        print(' '.join('%5s' % classes[labels[j]] for j in range(4)))\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Train Size: 50000, Test Size: 10000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n",
      "Train Size: 46000, Test Size: 10000\n",
      "Classes: ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_data(show=False, imbalanced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x1418068b0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ CHECKING TRAIN ------\n",
      "tensor([3, 6, 7, 4, 7, 1, 6, 7, 7, 8, 3, 5, 5, 5, 5, 0, 2, 2, 0, 6, 7, 5, 4, 0,\n",
      "        5, 5, 0, 4, 6, 7, 0, 3, 6, 6, 7, 1, 2, 2, 8, 0, 4, 7, 4, 3, 1, 2, 8, 7,\n",
      "        2, 1, 7, 4, 5, 0, 6, 6, 8, 7, 3, 6, 8, 6, 0, 4, 7, 7, 1, 5, 1, 3, 7, 7,\n",
      "        1, 5, 6, 7, 1, 6, 1, 0, 7, 7, 1, 3, 2, 5, 0, 7, 8, 9, 6, 2, 7, 5, 1, 8,\n",
      "        7, 7, 1, 3, 2, 3, 4, 1, 1, 0, 6, 1, 0, 5, 0, 6, 4, 2, 0, 8, 1, 8, 3, 5,\n",
      "        6, 2, 1, 3, 2, 7, 7, 5])\n",
      "tensor([1, 5, 7, 6, 0, 3, 5, 3, 5, 2, 5, 7, 8, 5, 7, 1, 2, 4, 9, 5, 0, 5, 3, 3,\n",
      "        6, 7, 5, 6, 8, 3, 7, 6, 7, 6, 4, 5, 3, 6, 7, 3, 4, 4, 0, 5, 0, 6, 6, 9,\n",
      "        6, 8, 8, 4, 2, 5, 1, 3, 6, 2, 3, 2, 4, 5, 0, 1, 2, 5, 3, 8, 5, 2, 1, 4,\n",
      "        8, 7, 7, 3, 6, 3, 5, 1, 0, 4, 6, 5, 8, 1, 2, 9, 0, 2, 0, 3, 0, 8, 1, 7,\n",
      "        4, 7, 9, 7, 8, 7, 5, 6, 2, 8, 5, 0, 4, 1, 4, 1, 6, 3, 4, 6, 6, 8, 3, 1,\n",
      "        6, 8, 0, 2, 7, 5, 6, 2])\n",
      "tensor([7, 2, 3, 3, 8, 2, 8, 2, 2, 2, 3, 8, 5, 7, 0, 7, 0, 6, 5, 6, 0, 5, 0, 0,\n",
      "        2, 3, 3, 1, 9, 6, 6, 2, 2, 2, 1, 8, 9, 0, 4, 1, 7, 1, 0, 5, 1, 0, 4, 6,\n",
      "        4, 8, 3, 6, 5, 0, 5, 9, 6, 1, 5, 5, 7, 6, 3, 7, 3, 9, 7, 2, 5, 5, 1, 7,\n",
      "        1, 5, 1, 4, 6, 1, 0, 3, 4, 3, 7, 6, 6, 7, 7, 0, 5, 1, 0, 7, 8, 7, 5, 2,\n",
      "        8, 6, 3, 3, 0, 6, 7, 5, 9, 7, 5, 6, 2, 4, 2, 7, 8, 4, 8, 4, 8, 6, 2, 5,\n",
      "        8, 3, 4, 1, 3, 9, 9, 3])\n",
      "tensor([4, 2, 1, 1, 4, 6, 1, 1, 5, 8, 2, 1, 0, 7, 8, 3, 3, 1, 0, 3, 5, 0, 5, 7,\n",
      "        5, 0, 0, 5, 1, 3, 8, 4, 5, 8, 8, 0, 3, 1, 2, 8, 4, 4, 1, 7, 9, 2, 8, 3,\n",
      "        0, 8, 8, 7, 0, 8, 8, 1, 1, 5, 3, 2, 5, 7, 4, 0, 7, 2, 8, 3, 7, 4, 2, 4,\n",
      "        0, 6, 8, 5, 6, 4, 3, 0, 0, 7, 2, 1, 8, 3, 7, 6, 7, 2, 3, 4, 0, 3, 2, 4,\n",
      "        6, 5, 6, 4, 2, 7, 4, 7, 7, 6, 2, 8, 2, 3, 6, 8, 2, 0, 2, 7, 5, 7, 8, 6,\n",
      "        5, 6, 3, 0, 7, 1, 6, 0])\n",
      "------ CHECKING TEST ------\n",
      "tensor([3, 8, 8, 0, 6, 6, 1, 6, 3, 1, 0, 9, 5, 7, 9, 8, 5, 7, 8, 6, 7, 0, 4, 9,\n",
      "        5, 2, 4, 0, 9, 6, 6, 5, 4, 5, 9, 2, 4, 1, 9, 5, 4, 6, 5, 6, 0, 9, 3, 9,\n",
      "        7, 6, 9, 8, 0, 3, 8, 8, 7, 7, 4, 6, 7, 3, 6, 3, 6, 2, 1, 2, 3, 7, 2, 6,\n",
      "        8, 8, 0, 2, 9, 3, 3, 8, 8, 1, 1, 7, 2, 5, 2, 7, 8, 9, 0, 3, 8, 6, 4, 6,\n",
      "        6, 0, 0, 7, 4, 5, 6, 3, 1, 1, 3, 6, 8, 7, 4, 0, 6, 2, 1, 3, 0, 4, 2, 7,\n",
      "        8, 3, 1, 2, 8, 0, 8, 3])\n",
      "tensor([5, 2, 4, 1, 8, 9, 1, 2, 9, 7, 2, 9, 6, 5, 6, 3, 8, 7, 6, 2, 5, 2, 8, 9,\n",
      "        6, 0, 0, 5, 2, 9, 5, 4, 2, 1, 6, 6, 8, 4, 8, 4, 5, 0, 9, 9, 9, 8, 9, 9,\n",
      "        3, 7, 5, 0, 0, 5, 2, 2, 3, 8, 6, 3, 4, 0, 5, 8, 0, 1, 7, 2, 8, 8, 7, 8,\n",
      "        5, 1, 8, 7, 1, 3, 0, 5, 7, 9, 7, 4, 5, 9, 8, 0, 7, 9, 8, 2, 7, 6, 9, 4,\n",
      "        3, 9, 6, 4, 7, 6, 5, 1, 5, 8, 8, 0, 4, 0, 5, 5, 1, 1, 8, 9, 0, 3, 1, 9,\n",
      "        2, 2, 5, 3, 9, 9, 4, 0])\n",
      "tensor([3, 0, 0, 9, 8, 1, 5, 7, 0, 8, 2, 4, 7, 0, 2, 3, 6, 3, 8, 5, 0, 3, 4, 3,\n",
      "        9, 0, 6, 1, 0, 9, 1, 0, 7, 9, 1, 2, 6, 9, 3, 4, 6, 0, 0, 6, 6, 6, 3, 2,\n",
      "        6, 1, 8, 2, 1, 6, 8, 6, 8, 0, 4, 0, 7, 7, 5, 5, 3, 5, 2, 3, 4, 1, 7, 5,\n",
      "        4, 6, 1, 9, 3, 6, 6, 9, 3, 8, 0, 7, 2, 6, 2, 5, 8, 5, 4, 6, 8, 9, 9, 1,\n",
      "        0, 2, 2, 7, 3, 2, 8, 0, 9, 5, 8, 1, 9, 4, 1, 3, 8, 1, 4, 7, 9, 4, 2, 7,\n",
      "        0, 7, 0, 6, 6, 9, 0, 9])\n",
      "tensor([2, 8, 7, 2, 2, 5, 1, 2, 6, 2, 9, 6, 2, 3, 0, 3, 9, 8, 7, 8, 8, 4, 0, 1,\n",
      "        8, 2, 7, 9, 3, 6, 1, 9, 0, 7, 3, 7, 4, 5, 0, 0, 2, 9, 3, 4, 0, 6, 2, 5,\n",
      "        3, 7, 3, 7, 2, 5, 3, 1, 1, 4, 9, 9, 5, 7, 5, 0, 2, 2, 2, 9, 7, 3, 9, 4,\n",
      "        3, 5, 4, 6, 5, 6, 1, 4, 3, 4, 4, 3, 7, 8, 3, 7, 8, 0, 5, 7, 6, 0, 5, 4,\n",
      "        8, 6, 8, 5, 5, 9, 9, 9, 5, 0, 1, 0, 8, 1, 1, 8, 0, 2, 2, 0, 4, 6, 5, 4,\n",
      "        9, 4, 7, 9, 9, 4, 5, 6])\n",
      "------ CHECKING VALIDATION ------\n",
      "tensor([9, 8, 3, 3, 3, 5, 2, 1, 7, 5, 5, 2, 2, 3, 4, 4, 5, 3, 5, 1, 0, 4, 3, 1,\n",
      "        6, 0, 4, 3, 6, 1, 4, 6, 8, 3, 5, 6, 8, 8, 0, 6, 1, 2, 2, 7, 0, 1, 6, 6,\n",
      "        2, 8, 0, 7, 2, 3, 7, 8, 4, 9, 8, 8, 5, 1, 9, 5, 2, 0, 6, 3, 2, 5, 1, 3,\n",
      "        2, 7, 7, 5, 8, 8, 4, 6, 0, 4, 8, 1, 1, 7, 0, 1, 7, 5, 4, 0, 5, 5, 6, 4,\n",
      "        8, 3, 6, 5, 9, 2, 3, 0, 6, 9, 0, 0, 1, 5, 0, 0, 9, 7, 1, 3, 8, 1, 5, 6,\n",
      "        1, 8, 6, 1, 2, 3, 4, 3])\n",
      "tensor([2, 8, 8, 4, 1, 5, 0, 2, 5, 1, 1, 3, 6, 6, 0, 4, 8, 4, 1, 3, 0, 3, 6, 4,\n",
      "        0, 1, 1, 5, 0, 2, 6, 9, 0, 7, 6, 7, 6, 2, 2, 2, 1, 5, 0, 0, 3, 3, 6, 3,\n",
      "        6, 2, 2, 1, 3, 7, 3, 1, 0, 7, 7, 0, 3, 0, 8, 0, 1, 7, 4, 0, 7, 8, 1, 1,\n",
      "        9, 2, 8, 7, 1, 8, 5, 4, 4, 5, 1, 0, 0, 8, 1, 2, 2, 8, 1, 7, 0, 3, 2, 7,\n",
      "        2, 7, 6, 3, 4, 0, 4, 2, 4, 3, 5, 8, 3, 3, 2, 2, 4, 0, 4, 2, 7, 4, 5, 3,\n",
      "        3, 3, 0, 2, 3, 6, 1, 7])\n",
      "tensor([6, 1, 8, 8, 3, 5, 7, 9, 8, 5, 3, 8, 2, 5, 5, 4, 0, 2, 1, 6, 6, 6, 0, 5,\n",
      "        5, 8, 0, 3, 0, 1, 8, 4, 7, 4, 7, 0, 3, 6, 3, 6, 3, 1, 8, 5, 8, 0, 1, 6,\n",
      "        1, 7, 6, 3, 6, 7, 2, 4, 8, 1, 5, 6, 7, 3, 3, 8, 0, 8, 0, 2, 4, 7, 8, 6,\n",
      "        2, 1, 2, 3, 4, 4, 3, 0, 7, 6, 8, 7, 8, 1, 8, 8, 0, 5, 5, 5, 2, 0, 8, 8,\n",
      "        8, 8, 7, 1, 4, 5, 1, 7, 0, 7, 1, 0, 0, 0, 3, 1, 5, 0, 3, 8, 1, 8, 6, 4,\n",
      "        0, 7, 8, 0, 1, 1, 7, 2])\n",
      "tensor([8, 0, 6, 3, 8, 0, 6, 2, 8, 5, 2, 7, 9, 7, 3, 0, 6, 5, 6, 5, 5, 5, 8, 6,\n",
      "        2, 2, 7, 4, 7, 6, 3, 3, 4, 6, 6, 4, 4, 8, 6, 5, 1, 8, 3, 3, 0, 4, 8, 2,\n",
      "        6, 2, 4, 5, 1, 3, 0, 2, 3, 5, 4, 5, 9, 4, 0, 4, 3, 8, 2, 6, 9, 6, 6, 6,\n",
      "        5, 3, 2, 8, 2, 5, 3, 1, 4, 1, 4, 8, 6, 3, 5, 2, 3, 4, 1, 4, 2, 5, 0, 1,\n",
      "        3, 5, 4, 8, 3, 6, 8, 3, 9, 7, 1, 6, 1, 4, 6, 0, 3, 0, 9, 9, 3, 2, 2, 6,\n",
      "        1, 7, 2, 2, 1, 8, 9, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"------ CHECKING TRAIN ------\")\n",
    "for i, (inputs, labels) in enumerate(train_loader):\n",
    "    if i > 3: \n",
    "        break\n",
    "    print(labels)\n",
    "    \n",
    "print(\"------ CHECKING TEST ------\")\n",
    "for i, (inputs, labels) in enumerate(test_loader):\n",
    "    if i > 3: \n",
    "        break\n",
    "    print(labels)\n",
    "\n",
    "print(\"------ CHECKING VALIDATION ------\")\n",
    "for i, (inputs, labels) in enumerate(val_loader):\n",
    "    if i > 3: \n",
    "        break\n",
    "    print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train():\n",
    "    torch.manual_seed(1)\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(),\n",
    "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    train_data = CIFAR10(train=True, download=True,\n",
    "                      root=\"../data\", transform=transform)\n",
    "\n",
    "    # creating imbalance for class #10 for the train data. \n",
    "    imbalanced_train = create_imbalance(train_data)\n",
    "\n",
    "    # splitting into a train and a validation \n",
    "    # creating the weighted sampler for both of these \n",
    "    # validation set is 4600 (10% of train), Train is 90%. \n",
    "    train_ds, val_ds = random_split(imbalanced_train, [41400, 4600])\n",
    "\n",
    "    # create sampler\n",
    "    sampler = adjust_imbalance_sampler(imbalanced_train)\n",
    "    train_loader = DataLoader(train_ds, batch_size=200, num_workers=4, sampler=sampler)\n",
    "    val_loader = DataLoader(val_ds, batch_size=200, sampler=sampler, num_workers=4)\n",
    "\n",
    "    test_data = CIFAR10(train=False, download=True,\n",
    "                        root=\"../data\", transform=transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_data, batch_size=200, num_workers=4, shuffle=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_imbalance_sampler(dataset):\n",
    "    print(dataset)\n",
    "    targets = dataset.targets\n",
    "    class_count = np.unique(targets, return_counts=True)[1]\n",
    "    print(class_count)\n",
    "\n",
    "    weight = 1. / class_count\n",
    "    samples_weight = weight[targets]\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 5000]\n",
      "imb class idx: [   29    30    35 ... 10178 10187 10198]\n",
      "46000\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "46000\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "After imbalance: None\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 46000\n",
      "    Root location: ../data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
      "           )\n",
      "[5000 5000 5000 5000 5000 5000 5000 5000 5000 1000]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = load_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x144a90880>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 257, in __getitem__\n    return self.dataset[self.indices[idx]]\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-39fc86b699af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.8/site-packages/torch/utils/data/dataset.py\", line 257, in __getitem__\n    return self.dataset[self.indices[idx]]\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "for batch, (inputs, labels) in enumerate(val_loader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
